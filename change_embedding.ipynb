{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q  transformers\n",
    "# from model.huggingface import HuggingFace\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cff13767cab466daf1407b5a1c4f6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load llama8b (4bitquantized)\n",
    "key = \"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\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, token=key, padding_side=\"left\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    token=key,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate a haiku with following theme:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# input_ids = tokenizer(task, return_tensors=\"pt\").input_ids.to(model.device)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     11\u001b[0m     messages,\n\u001b[1;32m     12\u001b[0m     add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     13\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m )\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     16\u001b[0m terminators \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39meos_token_id,\n\u001b[1;32m     18\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|eot_id|>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m ]\n\u001b[1;32m     21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     22\u001b[0m     input_ids,\n\u001b[1;32m     23\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     28\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"normal llama3 8b query\"\"\"\n",
    "messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "                {\"role\": \"user\", \"content\": \"create a haiku with following theme:\"},\n",
    "                {\"role\": \"user\", \"content\": \"Germany\"},\n",
    "            ]\n",
    "# messages = \"create a haiku with following theme:\"\n",
    "# input_ids = tokenizer(task, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12409/2417610784.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inject_embeddings = model.get_input_embeddings()(torch.tensor(encoding))\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[81236]])\n",
      "inject embedding size:\n",
      "torch.Size([1, 1, 4096])\n",
      "tensor([[128000, 128006,   9125, 128007,    271,   2675,    527,    264,  11190,\n",
      "          18328, 128009, 128006,    882, 128007,    271,   3261,    264,   6520,\n",
      "          39342,    449,   7057,     25, 128009, 128006,  78191, 128007,    271]],\n",
      "       device='cuda:0')\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "create a haiku with theme:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "embeddings size\n",
      "torch.Size([1, 27, 4096])\n",
      "inject_embeddings size\n",
      "torch.Size([1, 1, 4096])\n",
      "input_embeddings size\n",
      "torch.Size([1, 28, 4096])\n",
      "tensor([[ 28671,     74,   5754,  89248,  10437,    198,     47,  59952,    596,\n",
      "          14154,  14620,   2559,  16615,    198,  23078,    596,  11679,    292,\n",
      "           4851, 128009]], device='cuda:0')\n",
      "Silk road whispers sweet\n",
      "Peking's ancient walls stand tall\n",
      "China's mystic heart<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Define the explicit instruction and get its embeddings\n",
    "system_user_message = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "                {\"role\": \"user\", \"content\": \"create a haiku with theme:\"},\n",
    "            ]\n",
    "inject = \"china\"\n",
    "inject_input_ids = tokenizer(\n",
    "    inject,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "encoding = inject_input_ids['input_ids']\n",
    "print(encoding)\n",
    "# inject_input_ids = tokenizer(inject, add_special_tokens=False)['input_ids']\n",
    "\n",
    "inject_embeddings = model.get_input_embeddings()(torch.tensor(encoding))\n",
    "print(\"inject embedding size:\")\n",
    "print(inject_embeddings.shape)\n",
    "# Step 2: Tokenize the orisginal user message and get input embeddings\n",
    "system_input_ids = tokenizer.apply_chat_template(\n",
    "    system_user_message,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "print(system_input_ids)\n",
    "full_message = tokenizer.apply_chat_template(system_user_message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(full_message)\n",
    "\n",
    "system_embeddings = model.get_input_embeddings()(system_input_ids)\n",
    "print(\"embeddings size\")\n",
    "print(system_embeddings.size())\n",
    "print(\"inject_embeddings size\")\n",
    "print(inject_embeddings.size())\n",
    "# Step 3: Inject the explicit instruction embeddings into the input embeddings\n",
    "# Concatenate the explicit instruction embeddings with the original input embeddings\n",
    "insertion_position = 23  # Example position (modify as needed)\n",
    "injected_input_embeddings = torch.cat(\n",
    "    [system_embeddings[:, :insertion_position, :], inject_embeddings, system_embeddings[:, insertion_position:, :]],\n",
    "    dim=1\n",
    ")\n",
    "\n",
    "# injected_input_embeddings = torch.cat([system_embeddings,inject_embeddings], dim=1)\n",
    "print(\"input_embeddings size\")\n",
    "print(injected_input_embeddings.size())\n",
    "\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "# Step 4: Generate text using the modified embeddings\n",
    "outputs = model.generate(\n",
    "    inputs_embeds=injected_input_embeddings,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs)\n",
    "# Step 5: Decode the generated response\n",
    "\n",
    "response = outputs[0]#[injected_input_embeddings.shape[1]:]\n",
    "print(tokenizer.decode(response))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get input text and corresponding embeddings\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"What is capital of China?\"},\n",
    "# ]\n",
    "\n",
    "# # Tokenize input\n",
    "# input_ids = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(model.device)\n",
    "\n",
    "# # Get input embeddings and append the custom embedding\n",
    "# input_embeddings = model.get_input_embeddings()(input_ids).half()\n",
    "\n",
    "# # Identify the token IDs corresponding to the system message\n",
    "# msg_replacement =  {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"}\n",
    "# msg_replacement_input_ids = tokenizer.apply_chat_template(\n",
    "#     msg_replacement,\n",
    "#     add_generation_prompt=True,\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(model.device)\n",
    "\n",
    "# # Extract the embeddings generated by LLaMA's embedding layer\n",
    "# msg_replacement_input_embeddings = model.get_input_embeddings()(msg_replacement_input_ids)\n",
    "\n",
    "# system_message_ids = tokenizer(\"You are a pirate chatbot who always responds in pirate speak!\", return_tensors='pt').input_ids.to(model.device)\n",
    "# for token_id in system_message_ids[0]:\n",
    "#     input_embeddings[0][(input_ids[0] == token_id).nonzero(as_tuple=True)[0]] = msg_replacement_input_embeddings\n",
    "\n",
    "# # Generate text using the modified embeddings\n",
    "# outputs = model.generate(\n",
    "#     inputs_embeds=input_embeddings.half(),\n",
    "#     max_new_tokens=256,\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.6,\n",
    "#     top_p=0.9,\n",
    "# )\n",
    "# response = outputs[0][input_embeddings.shape[1]:]  \n",
    "# print(tokenizer.decode(response, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the explicit instruction and get its embeddings\n",
    "system_user_message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"create a haiku with following theme:\"},\n",
    "]\n",
    "\n",
    "# The inject content can be any string\n",
    "inject_content = \"Germany\"  # This will change dynamically\n",
    "inject = [{\"role\": \"user\", \"content\": inject_content}]\n",
    "\n",
    "# Step 2: Tokenize the inject message and get input IDs\n",
    "inject_input_ids = tokenizer.apply_chat_template(\n",
    "    inject,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# Get the embeddings for inject_input_ids\n",
    "inject_embeddings = model.get_input_embeddings()(inject_input_ids['input_ids'])\n",
    "\n",
    "# Step 3: Tokenize the inject content separately to get its token IDs\n",
    "inject_content_ids = tokenizer(inject_content, add_special_tokens=False)['input_ids']\n",
    "\n",
    "# Step 4: Identify the indices corresponding to the dynamic content in inject_input_ids\n",
    "# Get the list of token IDs from inject_input_ids\n",
    "inject_ids_list = inject_input_ids['input_ids'][0].tolist()\n",
    "\n",
    "# Function to find the indices of the inject content in inject_input_ids\n",
    "def find_content_indices(content_ids, input_ids):\n",
    "    content_len = len(content_ids)\n",
    "    for i in range(len(input_ids) - content_len + 1):\n",
    "        if input_ids[i:i+content_len] == content_ids:\n",
    "            return list(range(i, i+content_len))\n",
    "    return []\n",
    "\n",
    "# Find the indices where the inject content appears\n",
    "content_indices = find_content_indices(inject_content_ids, inject_ids_list)\n",
    "\n",
    "# Ensure we found the content indices\n",
    "if not content_indices:\n",
    "    raise ValueError(\"Inject content not found in inject_input_ids.\")\n",
    "\n",
    "# Step 5: Separate embeddings into static and dynamic content embeddings\n",
    "# Get all indices\n",
    "all_indices = list(range(len(inject_ids_list)))\n",
    "\n",
    "# Static content indices are those not in content_indices\n",
    "static_indices = [i for i in all_indices if i not in content_indices]\n",
    "\n",
    "# Extract static embeddings and dynamic embeddings\n",
    "static_embeddings = inject_embeddings[:, static_indices, :]\n",
    "dynamic_embeddings = inject_embeddings[:, content_indices, :]\n",
    "\n",
    "# Step 6: Mutate only the embeddings of the dynamic content\n",
    "def mutate_embeddings(embeddings):\n",
    "    # Example mutation: add Gaussian noise\n",
    "    noise = torch.randn_like(embeddings) * 0.01  # Adjust noise level as needed\n",
    "    return embeddings + noise\n",
    "\n",
    "mutated_dynamic_embeddings = mutate_embeddings(dynamic_embeddings)\n",
    "\n",
    "# Step 7: Reconstruct the inject_embeddings by combining static and mutated dynamic embeddings\n",
    "# Initialize a tensor to hold the reconstructed embeddings\n",
    "reconstructed_inject_embeddings = inject_embeddings.clone()\n",
    "\n",
    "# Place the mutated dynamic embeddings back into their positions\n",
    "for idx_embed, idx_content in enumerate(content_indices):\n",
    "    reconstructed_inject_embeddings[:, idx_content, :] = mutated_dynamic_embeddings[:, idx_embed, :]\n",
    "\n",
    "# Static embeddings remain unchanged\n",
    "\n",
    "# Step 8: Tokenize the original system_user_message and get input embeddings\n",
    "system_input_ids = tokenizer.apply_chat_template(\n",
    "    system_user_message,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "system_embeddings = model.get_input_embeddings()(system_input_ids['input_ids'])\n",
    "\n",
    "# Step 9: Concatenate the system embeddings and the reconstructed inject embeddings\n",
    "injected_input_embeddings = torch.cat([system_embeddings, reconstructed_inject_embeddings], dim=1)\n",
    "\n",
    "# Step 10: Generate text using the modified embeddings\n",
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs_embeds=injected_input_embeddings,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# Step 11: Decode the generated response\n",
    "response = outputs[0]\n",
    "print(tokenizer.decode(response))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qdllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
